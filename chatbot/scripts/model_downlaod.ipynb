{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e299fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abeee2c697d74b4e8699fa820bdec5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a4afc1e14248799afd6d8809bad71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/flan-t5-large/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af867a3227447329cb8ff92ad789592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  20%|#9        | 619M/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/flan-t5-large/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a8e797b3724128863784692cae7612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  50%|####9     | 1.56G/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/flan-t5-large/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3014c3f127439ea3a877ebffc98c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  57%|#####6    | 1.77G/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/flan-t5-large/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d1102a5e884820918bcbbd74dd11dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  81%|########1 | 2.55G/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84af16c1e9948cc924af4618dfd8545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9cf3d21130443a9f7eced96ab38024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb417f0ba9d94c148d5767c6ad5cfdf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c04aea2ceee4e64baeb45558d2e506f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61fe2a112c046a099852f80add002c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /Volumes/KODAK/folder 02/chatbot/model/base_model...\n",
      "Download and save completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Define your model and target path\n",
    "model_name = \"google/flan-t5-large\"\n",
    "save_path = \"/Volumes/KODAK/folder 02/chatbot/model/base_model\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Download and save the model and tokenizer\n",
    "print(\"Downloading model...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Saving model to {save_path}...\")\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"Download and save completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444788d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config: {'_name_or_path': 'google/flan-t5-large', 'architectures': ['T5ForConditionalGeneration'], 'classifier_dropout': 0.0, 'd_ff': 2816, 'd_kv': 64, 'd_model': 1024, 'decoder_start_token_id': 0, 'dense_act_fn': 'gelu_new', 'dropout_rate': 0.1, 'eos_token_id': 1, 'feed_forward_proj': 'gated-gelu', 'initializer_factor': 1.0, 'is_encoder_decoder': True, 'is_gated_act': True, 'layer_norm_epsilon': 1e-06, 'model_type': 't5', 'n_positions': 512, 'num_decoder_layers': 24, 'num_heads': 16, 'num_layers': 24, 'output_past': True, 'pad_token_id': 0, 'relative_attention_max_distance': 128, 'relative_attention_num_buckets': 32, 'tie_word_embeddings': False, 'torch_dtype': 'float32', 'transformers_version': '4.49.0', 'use_cache': True, 'vocab_size': 32128}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Check the config.json in your model directory\n",
    "config_path = \"/Volumes/KODAK/folder 02/chatbot/model/base_model/config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "print(\"Current config:\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48df91c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file updated successfully with model_type\n"
     ]
    }
   ],
   "source": [
    "# Update the config file with model_type\n",
    "config_path = \"/Volumes/KODAK/folder 02/chatbot/model/base_model/config.json\"\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Add the missing model_type for FLAN-T5\n",
    "config[\"model_type\"] = \"t5\"\n",
    "\n",
    "# Save the updated config\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Config file updated successfully with model_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b123dff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded with T5ForConditionalGeneration\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "model_path = \"/Volumes/KODAK/folder 02/chatbot/model/base_model\"\n",
    "\n",
    "# Load with explicit model class (recommended)\n",
    "try:\n",
    "    # Option 1: Load with explicit T5 class\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    print(\"Successfully loaded with T5ForConditionalGeneration\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading with explicit class: {str(e)}\")\n",
    "    \n",
    "    # Option 2: Manual loading as last resort\n",
    "    from transformers import PretrainedConfig, modeling_utils\n",
    "    \n",
    "    print(\"Attempting manual loading...\")\n",
    "    config = PretrainedConfig.from_pretrained(model_path)\n",
    "    config.model_type = \"t5\"  # Force set model type\n",
    "    \n",
    "    # Recreate the model\n",
    "    model = modeling_utils.model_from_config(\n",
    "        config,\n",
    "        trust_remote_code=True,\n",
    "        _fast_init=False\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    modeling_utils.load_state_dict_in_model(\n",
    "        model,\n",
    "        modeling_utils.safe_load_file(f\"{model_path}/model.safetensors\")\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    print(\"Manual loading completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671bc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
